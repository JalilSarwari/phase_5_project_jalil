{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'user_data/' + 'John' + '/' + 'John' + '_messages.json'\n",
    "f = open(filepath)\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "message = []\n",
    "# Iterating through the json\n",
    "# list\n",
    "for i in data:\n",
    "    try:\n",
    "        if i['media'] == None:\n",
    "            message.append(i['message'])\n",
    "    except:pass\n",
    "# Closing file\n",
    "f.close()\n",
    "f = open(\"output.txt\",\"a\")\n",
    "for i in data:\n",
    "    try:\n",
    "        if i['media'] == None:\n",
    "            f.write(i['message'] + '\\n')\n",
    "    except:pass\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file='output.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "\n",
    "    print('Vocabulary size', n_vocab)\n",
    "\n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "        flags.train_file, flags.batch_size, flags.seq_size)\n",
    "\n",
    "    net = RNNModule(n_vocab, flags.seq_size,\n",
    "                    flags.embedding_size, flags.lstm_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "    epochs = 200\n",
    "    for e in range(epochs):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            \n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, epochs),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "\n",
    "            if iteration % 10000 == 0:\n",
    "                predict(device, net, flags.initial_words, n_vocab,\n",
    "                        vocab_to_int, int_to_vocab, top_k=5)\n",
    "\n",
    "            if iteration % 50000 == 0:\n",
    "                torch.save(net.state_dict(),'model-{}.pth'.format(iteration))\n",
    "\n",
    "    torch.save(net.state_dict(),'final-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 11729\n",
      "Epoch: 6/200 Iteration: 1000 Loss: 4.091312408447266\n",
      "Epoch: 12/200 Iteration: 2000 Loss: 3.096794843673706\n",
      "Epoch: 19/200 Iteration: 3000 Loss: 2.4859397411346436\n",
      "Epoch: 25/200 Iteration: 4000 Loss: 2.2266616821289062\n",
      "Epoch: 32/200 Iteration: 5000 Loss: 2.08569073677063\n",
      "Epoch: 38/200 Iteration: 6000 Loss: 1.8503631353378296\n",
      "Epoch: 45/200 Iteration: 7000 Loss: 1.7102843523025513\n",
      "Epoch: 51/200 Iteration: 8000 Loss: 1.6224339008331299\n",
      "Epoch: 58/200 Iteration: 9000 Loss: 1.5227195024490356\n",
      "Epoch: 64/200 Iteration: 10000 Loss: 1.4663926362991333\n",
      "I am free on Sunday prob remember this morning, and talk for my comment ur thoughts It's just read it til 5/6 dumb Lol Omrahn please Screenshots Does Jalil Wdym? guys just going back for one to us, gross sides I genuinely thought u was telling the show but it's more accommodating already At eat Lol not even avg Why would I still play any game were watching this is my limits history that's waiting for deleting Brendan out now I'm not in Poor best speech to go see this U will know u like the busiest man after the same magic ready\n",
      "Epoch: 71/200 Iteration: 11000 Loss: 1.348059892654419\n",
      "Epoch: 77/200 Iteration: 12000 Loss: 1.4594959020614624\n",
      "Epoch: 84/200 Iteration: 13000 Loss: 1.4168871641159058\n",
      "Epoch: 90/200 Iteration: 14000 Loss: 1.4695709943771362\n",
      "Epoch: 97/200 Iteration: 15000 Loss: 1.2473317384719849\n",
      "Epoch: 103/200 Iteration: 16000 Loss: 1.0868231058120728\n",
      "Epoch: 110/200 Iteration: 17000 Loss: 1.5095093250274658\n",
      "Epoch: 116/200 Iteration: 18000 Loss: 1.2466437816619873\n",
      "Epoch: 123/200 Iteration: 19000 Loss: 1.4850302934646606\n",
      "Epoch: 129/200 Iteration: 20000 Loss: 1.4762285947799683\n",
      "I am free on Sunday prob remember this morning, and talk for my comment ur thoughts It's just read it til 5/6 dumb Lol Omrahn please Screenshots Does Jalil Wdym? guys just going back for one to us, gross sides I genuinely thought u was telling the show but it's more accommodating already At eat Lol not even avg Why would I still play any game were watching this is my limits history that's waiting for deleting Brendan out now I'm not in Poor best speech to go see this U will know u like the busiest man after the same magic ready That real read everything in my life @biggiethumbs Brendan kinda sick i just left well Good way No way one wanted to see if we start cutting a status after Tesla time with known u posts him No F but oh my e on meme videos Jamshid hassib just greg This man literally happened it up Get so fitting Damn it's an email sent or something with audio at its a fifa time to be in there an album flaws to watch any week and we don't appreciate it but Brendan Greg can't Jalil This are ASTRO wtf mins will never\n",
      "Epoch: 136/200 Iteration: 21000 Loss: 1.1536091566085815\n",
      "Epoch: 142/200 Iteration: 22000 Loss: 1.0612704753875732\n",
      "Epoch: 149/200 Iteration: 23000 Loss: 1.4376423358917236\n",
      "Epoch: 155/200 Iteration: 24000 Loss: 1.2109569311141968\n",
      "Epoch: 162/200 Iteration: 25000 Loss: 1.1604045629501343\n",
      "Epoch: 168/200 Iteration: 26000 Loss: 1.3134610652923584\n",
      "Epoch: 175/200 Iteration: 27000 Loss: 1.1692798137664795\n",
      "Epoch: 181/200 Iteration: 28000 Loss: 1.3597911596298218\n",
      "Epoch: 188/200 Iteration: 29000 Loss: 1.3438512086868286\n",
      "Epoch: 194/200 Iteration: 30000 Loss: 1.1806433200836182\n",
      "I am free on Sunday prob remember this morning, and talk for my comment ur thoughts It's just read it til 5/6 dumb Lol Omrahn please Screenshots Does Jalil Wdym? guys just going back for one to us, gross sides I genuinely thought u was telling the show but it's more accommodating already At eat Lol not even avg Why would I still play any game were watching this is my limits history that's waiting for deleting Brendan out now I'm not in Poor best speech to go see this U will know u like the busiest man after the same magic ready That real read everything in my life @biggiethumbs Brendan kinda sick i just left well Good way No way one wanted to see if we start cutting a status after Tesla time with known u posts him No F but oh my e on meme videos Jamshid hassib just greg This man literally happened it up Get so fitting Damn it's an email sent or something with audio at its a fifa time to be in there an album flaws to watch any week and we don't appreciate it but Brendan Greg can't Jalil This are ASTRO wtf mins will never man, really how the level 🤔 Now that and he's like him is cancelled Jalil so fucking good cuz there u got on the next Thursday bro guys only Like we improved how jet ones never gonna ask for over the same video How The magic other file and plz tyvm! Jalil and the Indian reaction too Lol ur chickens So cringe Jam of on it to the squad playing fifa of ya'll Philly to him At a beach game game past had response could forget hassib Another dead but it was playing on offense He @Jalil718 this can Wow u\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('final-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.7492, -0.0891,  0.8398,  ...,  1.8762, -1.0956, -0.6210],\n",
       "                      [-0.4163, -0.5728, -0.8769,  ..., -3.3641,  1.3001, -1.9331],\n",
       "                      [ 0.0447, -0.0503,  1.5316,  ..., -0.2947, -0.5491, -0.5677],\n",
       "                      ...,\n",
       "                      [-0.5741, -1.0428, -0.0182,  ..., -0.4536, -0.7862, -1.8054],\n",
       "                      [-0.1431,  0.1501, -0.3431,  ...,  0.0532,  1.3245,  2.3071],\n",
       "                      [ 0.0885,  0.8990, -1.2375,  ...,  0.2818,  0.0864, -0.3674]])),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.3429,  1.5110, -0.1029,  ..., -3.5115, -1.5791, -2.1300],\n",
       "                      [ 1.0831, -3.1639,  0.0602,  ..., -0.7969, -1.3242,  1.2067],\n",
       "                      [-1.1481, -0.9156,  0.8047,  ...,  0.1013, -0.3206,  1.4066],\n",
       "                      ...,\n",
       "                      [-2.6155, -0.1002,  0.4741,  ..., -3.3500,  1.4361,  0.6825],\n",
       "                      [-0.3810,  0.9341,  1.1572,  ..., -0.2089,  1.2531,  0.8514],\n",
       "                      [-1.2291, -1.0357,  1.9698,  ...,  1.6762, -1.4323,  0.2130]])),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[-1.6262,  1.4806, -1.2444,  ...,  1.4791, -0.3750, -2.5212],\n",
       "                      [-0.6970,  0.1753,  1.8396,  ..., -4.7846,  0.8382,  0.7409],\n",
       "                      [-2.3770, -1.2199, -1.9084,  ...,  0.9413, -1.1821,  0.4173],\n",
       "                      ...,\n",
       "                      [-4.8050, -0.2383,  2.6350,  ..., -2.6927, -0.6466, -0.9694],\n",
       "                      [-0.0168, -2.2059,  0.2117,  ..., -2.6538, -5.1043, -3.2791],\n",
       "                      [ 0.2023,  0.6342,  1.6213,  ...,  2.0566,  3.1409,  2.0311]])),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([-1.6527e-01,  1.3532e+00,  9.0047e-01,  1.7085e+00,  1.8864e+00,\n",
       "                       2.6616e+00,  2.3688e+00,  4.0330e+00,  2.4079e+00,  3.0092e+00,\n",
       "                       2.2276e+00,  3.8569e+00,  2.7333e+00,  2.2900e+00,  2.6637e+00,\n",
       "                       3.3904e+00,  3.5682e+00,  1.8931e+00,  2.9508e+00,  1.6966e+00,\n",
       "                       2.6715e+00,  3.4621e+00,  1.2961e+00,  2.7028e+00,  3.1287e+00,\n",
       "                       2.6809e+00,  3.8619e+00,  3.6659e+00,  3.3096e+00,  3.5982e+00,\n",
       "                       3.6935e+00,  2.8486e+00,  2.4197e+00,  3.0310e+00,  1.7433e+00,\n",
       "                       2.2215e+00,  1.4410e+00,  2.4688e+00,  6.4299e-01,  3.0696e+00,\n",
       "                       1.3463e+00,  2.7168e+00,  3.2399e+00,  1.9276e+00,  3.5745e+00,\n",
       "                       3.2884e+00,  1.6476e-01,  1.3118e+00,  2.6834e+00,  9.6356e-01,\n",
       "                       1.9800e+00,  1.4231e+00,  2.5760e+00,  3.2381e+00,  7.2460e-01,\n",
       "                       2.0630e+00,  1.7964e+00,  2.6468e+00,  5.2545e+00,  2.4370e+00,\n",
       "                       9.6260e-01,  1.5211e+00,  1.0612e+00,  2.4334e+00,  3.6597e+00,\n",
       "                       2.3161e+00,  1.8604e+00,  5.7968e-01, -1.2828e+00,  2.2637e+00,\n",
       "                       3.9309e+00, -8.9942e-01,  3.4189e+00,  4.1277e+00,  1.4256e+00,\n",
       "                      -1.6719e+00,  3.6203e+00, -2.2791e+00, -9.0607e-01,  2.4215e+00,\n",
       "                       5.9262e-01, -9.2273e-02, -1.0293e+00,  1.9685e+00,  2.4495e+00,\n",
       "                       3.3560e-01, -2.2517e-01,  3.8571e+00,  1.7132e+00,  3.9753e+00,\n",
       "                       2.7817e+00,  1.1049e+00, -2.0086e+00,  4.5225e+00,  7.9183e-01,\n",
       "                       2.1755e+00,  9.7367e-01,  7.4328e-01, -1.5931e+00, -1.9530e+00,\n",
       "                      -9.1653e-01,  3.1599e+00,  7.3734e-01,  2.6872e+00,  1.8615e+00,\n",
       "                       2.5907e+00,  1.0863e+00,  1.2166e+00,  8.6888e-01,  6.0189e-01,\n",
       "                       1.8118e+00,  1.7418e+00,  1.8045e+00,  4.0427e-01,  2.0138e-01,\n",
       "                       2.5891e+00,  3.1466e+00,  2.3163e-01, -1.1129e+00,  3.2798e+00,\n",
       "                       3.9302e+00, -1.8364e+00, -1.0742e+00,  2.4961e+00,  1.4237e+00,\n",
       "                       1.2336e+00,  3.8265e+00,  2.4181e+00, -3.8783e+00,  2.1104e+00,\n",
       "                      -2.2173e+00,  2.7261e-01,  2.9230e-01,  3.5328e+00, -3.7880e+00,\n",
       "                       8.2378e-01,  3.9686e+00, -4.1589e+00, -1.3274e+00,  4.0352e-01,\n",
       "                      -5.1298e+00,  7.2272e-01, -1.2929e-01, -3.5196e+00,  2.3283e+00,\n",
       "                      -1.1590e+00,  1.1510e+00,  2.9905e+00, -2.4824e+00,  3.0505e+00,\n",
       "                       1.1299e+00, -4.1053e+00,  3.0353e+00,  3.2544e+00,  2.7064e+00,\n",
       "                       3.2706e+00, -3.3397e-01,  3.7614e+00,  1.0709e+00, -3.1431e+00,\n",
       "                      -2.4118e+00, -1.0106e+00,  5.2428e-01, -1.8421e-01,  9.1606e-01,\n",
       "                      -5.0894e+00,  2.2669e+00, -4.4309e+00, -4.1330e+00,  2.4588e+00,\n",
       "                      -2.4954e+00, -4.2463e+00,  3.2252e+00,  8.2135e-01,  6.4856e+00,\n",
       "                      -8.3329e-01,  2.3401e+00, -2.1097e+00,  1.0434e+00,  3.1602e+00,\n",
       "                      -1.5711e+00, -9.8421e-01,  1.2861e-02, -4.3035e+00, -3.4262e+00,\n",
       "                      -4.0864e-01, -2.2883e-01, -3.8514e+00,  2.4498e+00,  2.3987e+00,\n",
       "                      -2.9888e+00,  4.6366e+00, -1.0524e+00, -1.8618e+00, -3.9606e-01,\n",
       "                       8.6218e-01,  5.9665e-01,  7.6470e-01,  8.9209e-02,  1.7751e+00,\n",
       "                      -1.2389e+00, -1.7010e+00, -1.5839e+00,  3.9485e+00, -4.3705e-01,\n",
       "                       3.8417e+00,  2.0359e+00, -1.0985e+00, -5.0371e-01, -4.2381e-01,\n",
       "                       7.9822e-02, -4.1979e-01,  9.2570e-01,  1.5452e+00,  2.8127e+00,\n",
       "                      -1.2959e+00, -3.1910e-01, -5.7505e-01, -7.7066e-01, -1.1212e+00,\n",
       "                       1.4508e+00, -3.5215e-01, -6.9811e-01, -1.3001e-01, -1.9372e+00,\n",
       "                      -6.2390e-01,  5.0999e-01,  1.8595e+00, -1.3578e-01, -1.7810e+00,\n",
       "                      -8.4950e-01, -9.1057e-01, -7.5921e-01, -2.4808e+00, -1.2320e+00,\n",
       "                      -2.2502e+00, -7.5280e-01,  4.0669e-01, -2.0337e-02,  8.1490e-01,\n",
       "                      -6.1442e-01,  3.0722e+00,  2.9387e+00, -5.2459e-01, -1.6081e+00,\n",
       "                       5.4019e-01,  2.0057e+00, -9.6343e-01, -6.6761e-01,  4.2041e+00,\n",
       "                       3.6990e+00,  7.0295e-03, -1.4545e-03, -8.7932e-01, -2.5251e-01,\n",
       "                      -6.9513e-01])),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([-2.4986e-01,  1.3838e+00,  1.0103e+00,  1.4765e+00,  1.9260e+00,\n",
       "                       2.4766e+00,  2.4998e+00,  4.1687e+00,  2.2075e+00,  2.9716e+00,\n",
       "                       2.3870e+00,  3.8287e+00,  2.8430e+00,  2.1324e+00,  2.6401e+00,\n",
       "                       3.2699e+00,  3.6250e+00,  1.8963e+00,  3.0359e+00,  1.5823e+00,\n",
       "                       2.4869e+00,  3.5705e+00,  1.2672e+00,  2.7258e+00,  3.2291e+00,\n",
       "                       2.6422e+00,  3.9628e+00,  3.7718e+00,  3.3772e+00,  3.5940e+00,\n",
       "                       3.6348e+00,  2.6796e+00,  2.4298e+00,  3.1772e+00,  1.8513e+00,\n",
       "                       2.2401e+00,  1.4786e+00,  2.3466e+00,  6.3707e-01,  3.2624e+00,\n",
       "                       1.3846e+00,  2.6392e+00,  3.2385e+00,  1.8898e+00,  3.4429e+00,\n",
       "                       3.2378e+00,  2.1963e-01,  1.3807e+00,  2.7803e+00,  1.0812e+00,\n",
       "                       1.8743e+00,  1.4925e+00,  2.5209e+00,  3.3117e+00,  7.5221e-01,\n",
       "                       1.9908e+00,  1.7571e+00,  2.7833e+00,  5.2363e+00,  2.3858e+00,\n",
       "                       8.7085e-01,  1.5238e+00,  1.1342e+00,  2.4083e+00,  3.5713e+00,\n",
       "                       2.3255e+00,  1.8347e+00,  5.9884e-01, -1.2157e+00,  2.4213e+00,\n",
       "                       3.9507e+00, -1.0257e+00,  3.4086e+00,  3.9370e+00,  1.4198e+00,\n",
       "                      -1.7710e+00,  3.4836e+00, -2.3401e+00, -1.0089e+00,  2.5164e+00,\n",
       "                       4.8894e-01, -1.3171e-02, -9.0073e-01,  1.8342e+00,  2.4833e+00,\n",
       "                       2.9572e-01, -1.8986e-02,  3.9648e+00,  1.6884e+00,  3.8635e+00,\n",
       "                       2.8706e+00,  1.0496e+00, -1.9805e+00,  4.4612e+00,  9.2836e-01,\n",
       "                       2.1775e+00,  1.0060e+00,  6.1197e-01, -1.4688e+00, -1.7249e+00,\n",
       "                      -7.9536e-01,  2.9966e+00,  6.7654e-01,  2.6593e+00,  1.7698e+00,\n",
       "                       2.6577e+00,  1.2440e+00,  1.3872e+00,  8.8762e-01,  7.7283e-01,\n",
       "                       1.9658e+00,  1.8555e+00,  1.7820e+00,  2.5122e-01,  1.7883e-01,\n",
       "                       2.6609e+00,  3.0026e+00,  2.7457e-01, -1.1025e+00,  3.2459e+00,\n",
       "                       3.9473e+00, -1.6835e+00, -1.1160e+00,  2.3962e+00,  1.4628e+00,\n",
       "                       1.0120e+00,  3.6831e+00,  2.4232e+00, -3.9457e+00,  2.3075e+00,\n",
       "                      -2.1400e+00,  3.8869e-01,  2.7569e-01,  3.6339e+00, -3.7975e+00,\n",
       "                       1.0260e+00,  3.9085e+00, -4.2649e+00, -1.4693e+00,  3.1040e-01,\n",
       "                      -4.9861e+00,  5.0749e-01, -5.0544e-02, -3.5043e+00,  2.5290e+00,\n",
       "                      -1.1878e+00,  1.1480e+00,  2.9337e+00, -2.6579e+00,  2.9389e+00,\n",
       "                       1.0035e+00, -4.2041e+00,  3.0607e+00,  3.3958e+00,  2.8440e+00,\n",
       "                       3.2009e+00, -4.0278e-01,  3.7230e+00,  1.1895e+00, -3.2598e+00,\n",
       "                      -2.4627e+00, -9.7902e-01,  7.2228e-01, -3.9114e-01,  8.8211e-01,\n",
       "                      -5.0315e+00,  2.2265e+00, -4.2900e+00, -4.2150e+00,  2.5841e+00,\n",
       "                      -2.6714e+00, -4.0540e+00,  3.0116e+00,  8.7630e-01,  6.5489e+00,\n",
       "                      -7.6131e-01,  2.3145e+00, -2.1241e+00,  1.2284e+00,  3.1867e+00,\n",
       "                      -1.4990e+00, -8.8383e-01, -1.0734e-02, -4.4658e+00, -3.4400e+00,\n",
       "                      -2.1650e-01, -3.9977e-01, -3.7188e+00,  2.4825e+00,  2.3545e+00,\n",
       "                      -3.0950e+00,  4.7003e+00, -9.5292e-01, -1.8820e+00, -4.9660e-01,\n",
       "                       9.8324e-01,  5.3791e-01,  7.4011e-01,  9.5952e-02,  1.8565e+00,\n",
       "                      -1.2694e+00, -1.7320e+00, -1.6950e+00,  4.0323e+00, -4.9335e-01,\n",
       "                       3.8564e+00,  2.1050e+00, -1.1209e+00, -5.5324e-01, -3.3118e-01,\n",
       "                       9.1181e-02, -4.0335e-01,  8.0355e-01,  1.4205e+00,  2.7366e+00,\n",
       "                      -1.4531e+00, -2.9523e-01, -4.9913e-01, -9.1551e-01, -1.1199e+00,\n",
       "                       1.5073e+00, -2.7629e-01, -6.3031e-01, -1.8248e-03, -1.7786e+00,\n",
       "                      -7.4336e-01,  3.8535e-01,  1.7177e+00,  2.2201e-03, -1.7826e+00,\n",
       "                      -9.6909e-01, -8.2418e-01, -7.0246e-01, -2.3385e+00, -1.3372e+00,\n",
       "                      -2.4051e+00, -7.3867e-01,  4.3960e-01, -5.1761e-02,  9.0959e-01,\n",
       "                      -6.1520e-01,  2.9604e+00,  2.7636e+00, -7.4311e-01, -1.5752e+00,\n",
       "                       6.8864e-01,  1.9283e+00, -9.3208e-01, -7.4057e-01,  4.0975e+00,\n",
       "                       3.9080e+00,  3.1266e-02, -8.3870e-02, -1.0426e+00, -3.8176e-01,\n",
       "                      -6.2335e-01])),\n",
       "             ('dense.weight',\n",
       "              tensor([[-1.1236,  1.1093, -0.9253,  ...,  0.2759, -1.1340,  1.2226],\n",
       "                      [-0.1720,  1.6963,  0.6430,  ...,  1.0709,  0.0059, -1.8879],\n",
       "                      [ 0.3033,  0.5898, -0.0876,  ...,  0.2695,  0.0489, -1.1778],\n",
       "                      ...,\n",
       "                      [ 0.8013, -1.0295,  0.9645,  ..., -0.7507,  0.9416, -1.1782],\n",
       "                      [ 0.8991, -1.0490,  0.9252,  ..., -0.7976,  1.1062, -1.1902],\n",
       "                      [ 0.9166, -1.0160,  0.9482,  ..., -0.7776,  1.1123, -1.2504]])),\n",
       "             ('dense.bias',\n",
       "              tensor([ 0.7059,  1.2951,  1.4436,  ..., -1.5751, -1.4617, -1.4428]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
